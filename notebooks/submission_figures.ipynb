{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-13T19:17:37.813114Z",
     "start_time": "2024-06-13T19:17:37.584185Z"
    }
   },
   "source": [
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import gzip\n",
    "from sklearn.inspection import permutation_importance\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, RegularPolygon\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.projections.polar import PolarAxes\n",
    "from matplotlib.projections import register_projection\n",
    "from matplotlib.spines import Spine\n",
    "from matplotlib.transforms import Affine2D"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T11:21:27.226348Z",
     "start_time": "2024-06-11T11:21:27.222231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to load a gzipped model\n",
    "def load_model(gzip_path):\n",
    "    with gzip.GzipFile(gzip_path, \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "    \n",
    "    # with gzip.open(filename, 'rb') as f:\n",
    "    #     model = joblib.load(f)\n",
    "    return model\n",
    "\n",
    "# Function to plot feature importances\n",
    "def plot_feature_importances(importances, classifier_name, feature_names, top_n=20):\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    top_indices = indices[:top_n]\n",
    "    sorted_feature_names = [feature_names[i] for i in top_indices]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(top_n), importances[top_indices], align='center')\n",
    "    plt.xticks(range(top_n), sorted_feature_names, rotation=90)\n",
    "    plt.title(f'Top {top_n} Feature Importances - {classifier_name}')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "4107494ec2aae3e0",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T11:21:34.842371Z",
     "start_time": "2024-06-11T11:21:34.839636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classifiers = {\n",
    "            \"Logistic_Regression\": 'Logistic_Regression_BE.sav.gz',\n",
    "            \"Gaussian_NB\":'Gaussian_NB_BE.sav.gz',\n",
    "            # \"Bernoulli_NB\":'Bernoulli_NB_BE.sav.gz',\n",
    "            \"Decision_Tree\":'Decision_Tree_BE.sav.gz',\n",
    "            \"Random_Forest\":'Random_Forest_BE.sav.gz',\n",
    "            \"Extra_Trees_Classifier\":'Extra_Trees_Classifier_BE.sav.gz',\n",
    "            \"Ada_Boost\":'Ada_Boost_BE.sav.gz',\n",
    "            \"Gradient_Boosting\":'Gradient_Boosting_BE.sav.gz',\n",
    "            \"LightGBM\":'LightGBM_BE.sav.gz',\n",
    "            \"Hist_GB\":'Hist_GB_BE.sav.gz',\n",
    "            \"XGBoost\":'XGBoost_BE.sav.gz',\n",
    "}"
   ],
   "id": "39eb59929e120a63",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T07:07:54.629419Z",
     "start_time": "2024-06-11T07:07:54.510588Z"
    }
   },
   "cell_type": "code",
   "source": "response_data = pd.read_parquet('../data/processed/chrome/08_12_2022/test_set_featurized_response_BE.parquet.gzip', engine='pyarrow', dtype_backend='pyarrow')",
   "id": "17b4b6effdd51adb",
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T11:55:00.018337Z",
     "start_time": "2024-06-11T11:54:59.996563Z"
    }
   },
   "cell_type": "code",
   "source": "request_data = pd.read_parquet('../data/processed/firefox/08_12_2022/merged_data_featurized_request_BE.parquet.gzip', engine='pyarrow', dtype_backend='pyarrow')",
   "id": "f5f14456def7daf",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T11:55:00.953257Z",
     "start_time": "2024-06-11T11:55:00.938148Z"
    }
   },
   "cell_type": "code",
   "source": "request_data",
   "id": "72868717f16bcc58",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T07:07:58.428875Z",
     "start_time": "2024-06-11T07:07:58.306596Z"
    }
   },
   "cell_type": "code",
   "source": "response_data",
   "id": "a81b1a5327fe426e",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T11:54:47.916626Z",
     "start_time": "2024-06-11T11:54:47.903250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = response_data.iloc[:, 2:-2]  # All columns except the last\n",
    "y = response_data.iloc[:, -1]   # The last column\n",
    "feature_names = X.columns.tolist()"
   ],
   "id": "ce111cc0bffb202d",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T11:55:06.713127Z",
     "start_time": "2024-06-11T11:55:06.709443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = request_data.iloc[:, 2:-2]  # All columns except the last\n",
    "y = request_data.iloc[:, -1]   # The last column\n",
    "feature_names = X.columns.tolist()"
   ],
   "id": "a4b15635fe62715b",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T07:08:20.896169Z",
     "start_time": "2024-06-11T07:08:10.885676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Load the models and plot feature importances\n",
    "# for name, file in classifiers.items():\n",
    "#     gzip_path = f\"../models/chrome/08_12_2022/response/{file}\"\n",
    "#     clf = load_model(gzip_path)\n",
    "#     if hasattr(clf, 'feature_importances_'):\n",
    "#         importances = clf.feature_importances_\n",
    "#         plot_feature_importances(importances, name, feature_names)\n",
    "#     else:\n",
    "#         # Calculate permutation importance\n",
    "#         result = permutation_importance(clf, X, y, n_repeats=10, random_state=10, n_jobs=-1)\n",
    "#         importances = result.importances_mean\n",
    "#         plot_feature_importances(importances, name, feature_names)"
   ],
   "id": "4f7cd17b1cff6252",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T19:04:05.671052Z",
     "start_time": "2024-05-26T19:04:04.711136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Plot heatmap with a diverging color palette\n",
    "# plt.figure(figsize=(16, 10))\n",
    "# sns.set(font_scale=1.2)\n",
    "# heatmap = sns.heatmap(\n",
    "#     feature_importances_top_df,\n",
    "#     annot=True,\n",
    "#     cmap='RdBu_r',\n",
    "#     center=0,\n",
    "#     linewidths=.5,\n",
    "#     cbar_kws={'label': 'Normalized Importance'},\n",
    "#     fmt='.3f',\n",
    "#     annot_kws={\"size\": 10}\n",
    "# )\n",
    "# heatmap.set_title('Top 20 Feature Importances Across Classifiers', fontsize=16)\n",
    "# heatmap.set_xlabel('Classifier', fontsize=14)\n",
    "# heatmap.set_ylabel('Feature', fontsize=14)\n",
    "# plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "# plt.yticks(fontsize=12)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('feature_importances_heatmap.pdf', format='pdf')\n",
    "# plt.show()"
   ],
   "id": "47e12938a3cd65d",
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T11:59:11.994035Z",
     "start_time": "2024-06-11T11:55:13.819555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# Initialize a DataFrame to hold feature importances\n",
    "feature_importances_dict = {name: pd.Series(dtype=float) for name in classifiers}\n",
    "\n",
    "# Load the models and compute feature importances\n",
    "for name, file in classifiers.items():\n",
    "    gzip_path = f\"../models/chrome/08_12_2022/request/{file}\" # do not forget to change filename to response or request \n",
    "    clf = load_model(gzip_path)\n",
    "    if hasattr(clf, 'feature_importances_'):\n",
    "        importances = clf.feature_importances_\n",
    "    else:\n",
    "        # Calculate permutation importance\n",
    "        result = permutation_importance(clf, X, y, n_repeats=10, random_state=10, n_jobs=-1)\n",
    "        importances = result.importances_mean\n",
    "    \n",
    "    # Normalize importances\n",
    "    importances_normalized = importances / np.sum(importances)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    feature_importances_dict[name] = pd.Series(importances_normalized, index=feature_names)\n",
    "\n",
    "# Combine top features from all classifiers\n",
    "combined_features = pd.concat(feature_importances_dict.values()).groupby(level=0).sum()\n",
    "top_combined_features = combined_features.nlargest(20).index"
   ],
   "id": "2a86af4e46912842",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:40:53.406090Z",
     "start_time": "2024-06-11T12:40:53.402880Z"
    }
   },
   "cell_type": "code",
   "source": "top_combined_features = combined_features.nlargest(10).index",
   "id": "5fa1d30241c2625f",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T07:34:46.067170Z",
     "start_time": "2024-06-11T07:34:45.380858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create DataFrame for heatmap\n",
    "feature_importances_top_df = pd.DataFrame({name: feature_importances_dict[name].reindex(top_combined_features).fillna(0) for name in classifiers})\n",
    "\n",
    "# Function to insert a newline character after each '-'\n",
    "def insert_line_breaks(text):\n",
    "    return text.replace('-', '-\\n')\n",
    "\n",
    "# Transform feature names for display and add line breaks for long names\n",
    "feature_importances_top_df.index = feature_importances_top_df.index.str.replace('_binary', '').str.replace('_', ' ').str.title()\n",
    "feature_importances_top_df.index = feature_importances_top_df.index.map(insert_line_breaks)\n",
    "\n",
    "# feature_importances_top_df.index = feature_importances_top_df.index.str.replace('_binary', '').str.replace('_', ' ').str.title()\n",
    "\n",
    "\n",
    "\n",
    "# Plot heatmap with a diverging color palette\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.set(font_scale=1.2)\n",
    "heatmap = sns.heatmap(\n",
    "    feature_importances_top_df,\n",
    "    annot=True,\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    linewidths=.5,\n",
    "    cbar_kws={'label': 'Normalized Importance'},\n",
    "    fmt='.3f',\n",
    "    annot_kws={\"size\": 12, \"weight\": \"bold\"}\n",
    ")\n",
    "custom_labels = [\"LR\", \"GNB\", \"DT\", \"RF\", \"ET\", \"AdaBoost\", \"GBM\", \"LightGBM\", \"HistGB\", \"XGBoost\"]\n",
    "\n",
    "heatmap.set_title('Top 10 Normalized Feature Importances Across Classifiers', fontsize=16)\n",
    "heatmap.set_xlabel('Classifier', fontsize=14)\n",
    "heatmap.set_ylabel('Feature', fontsize=14)\n",
    "# plt.xticks(ticks=np.arange(len(custom_labels)), labels=custom_labels, rotation=90, ha='center', fontsize=12)\n",
    "heatmap.set_xticklabels(custom_labels, rotation=0, ha='center', fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# plt.savefig('request_performance_comparison.pdf', bbox_inches='tight', pad_inches=0.1)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importances_heatmap.pdf', format='pdf', bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "id": "4dc46a01832e680",
   "execution_count": 51,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:41:04.321903Z",
     "start_time": "2024-06-11T12:41:03.547307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create DataFrame for heatmap\n",
    "feature_importances_top_df = pd.DataFrame({name: feature_importances_dict[name].reindex(top_combined_features).fillna(0) for name in classifiers})\n",
    "\n",
    "# Function to insert a newline character after each '-'\n",
    "def insert_line_breaks(text):\n",
    "    return text.replace('-', '-\\n')\n",
    "\n",
    "# Transform feature names for display and add line breaks for long names\n",
    "feature_importances_top_df.index = feature_importances_top_df.index.str.replace('_binary', '').str.replace('_', ' ').str.title()\n",
    "feature_importances_top_df.index = feature_importances_top_df.index.map(insert_line_breaks)\n",
    "\n",
    "# feature_importances_top_df.index = feature_importances_top_df.index.str.replace('_binary', '').str.replace('_', ' ').str.title()\n",
    "\n",
    "\n",
    "\n",
    "# Plot heatmap with a diverging color palette\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.set(font_scale=1.2)\n",
    "heatmap = sns.heatmap(\n",
    "    feature_importances_top_df,\n",
    "    annot=True,\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    linewidths=.5,\n",
    "    cbar_kws={'label': 'Normalized Importance'},\n",
    "    fmt='.3f',\n",
    "    annot_kws={\"size\": 12, \"weight\": \"bold\"}\n",
    ")\n",
    "custom_labels = [\"LR\", \"GNB\", \"DT\", \"RF\", \"ET\", \"AdaBoost\", \"GBM\", \"LightGBM\", \"HistGB\", \"XGBoost\"]\n",
    "\n",
    "heatmap.set_title('Top 10 Normalized Feature Importances Across Request Header Classifiers', fontsize=16)\n",
    "heatmap.set_xlabel('Classifier', fontsize=14)\n",
    "heatmap.set_ylabel('Feature', fontsize=14)\n",
    "# plt.xticks(ticks=np.arange(len(custom_labels)), labels=custom_labels, rotation=90, ha='center', fontsize=12)\n",
    "heatmap.set_xticklabels(custom_labels, rotation=0, ha='center', fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# plt.savefig('request_performance_comparison.pdf', bbox_inches='tight', pad_inches=0.1)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importances_heatmap_req.pdf', format='pdf', bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "id": "efcc5322520eca51",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T07:33:50.504553Z",
     "start_time": "2024-05-27T07:33:50.496370Z"
    }
   },
   "cell_type": "code",
   "source": "response_data[['cross-origin-resource-policy_binary', 'tracker']].groupby('tracker').count()",
   "id": "fe83f8823021043",
   "execution_count": 57,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T11:29:51.741981Z",
     "start_time": "2024-05-27T11:29:51.735041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gzip_path = f\"../models/chrome/08_12_2022/response/LightGBM_BE.sav.gz\"\n",
    "clf = load_model(gzip_path)"
   ],
   "id": "b3924363e1f4ad84",
   "execution_count": 58,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T11:30:09.028107Z",
     "start_time": "2024-05-27T11:30:09.024520Z"
    }
   },
   "cell_type": "code",
   "source": "clf.feature_importances_ / np.sum(clf.feature_importances_)",
   "id": "bb13bec8a512e9b8",
   "execution_count": 60,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Frequency Plot",
   "id": "7d417c4f4442b19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "chrome_old = pd.read_parquet(\n",
    "    \"../data/processed/chrome/08_12_2022/merged_data.parquet.gzip\", engine=\"pyarrow\",\n",
    "            dtype_backend=\"pyarrow\"\n",
    ")\n",
    "\n",
    "chrome_new = pd.read_parquet('../data/processed/chrome/03_29_2023/merged_data.parquet.gzip', engine=\"pyarrow\", dtype_backend=\"pyarrow\")\n",
    "\n",
    "firefox = pd.read_parquet('../data/processed/firefox/08_12_2022/merged_data.parquet.gzip', engine=\"pyarrow\",\n",
    "            dtype_backend=\"pyarrow\")\n",
    "\n",
    "brave = pd.read_parquet('../data/processed/brave/08_12_2022/merged_data.parquet.gzip', engine=\"pyarrow\",\n",
    "            dtype_backend=\"pyarrow\")"
   ],
   "id": "835d8eb35c3a3ed9",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T07:04:19.180744Z",
     "start_time": "2024-06-11T07:04:18.758792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the datasets\n",
    "baseline_file_path = '../models/result_metrics/t.ex-Graph (FQDN)-pretty.csv'\n",
    "http_response_file_path = '../models/result_metrics/chrome_08_12_2022_test_set_response.csv'\n",
    "http_request_file_path = '../models/result_metrics/chrome_08_12_2022_test_set_request.csv'\n",
    "\n",
    "baseline_data = pd.read_csv(baseline_file_path)\n",
    "http_response_data = pd.read_csv(http_response_file_path)\n",
    "http_request_data = pd.read_csv(http_request_file_path)\n",
    "\n",
    "# Extracting Random Forest data for the three datasets\n",
    "baseline_rf = baseline_data[baseline_data['t.ex-Graph (FQDN)'] == 'RandomForestClassifier']\n",
    "http_response_rf = http_response_data[http_response_data['Unnamed: 0'] == 'Random_Forest']\n",
    "http_request_rf = http_request_data[http_request_data['Unnamed: 0'] == 'Random_Forest']\n",
    "\n",
    "# Select and rename columns for consistency\n",
    "baseline_rf_selected = baseline_rf[['logloss', 'auprc', 'balanced_accuracy', 'f1_score', 'precision', 'recall', 'matthews_corrcoef']]\n",
    "baseline_rf_selected.columns = ['Log loss', 'AUPRC', 'BACC', 'F1', 'Precision', 'Recall', 'MCC']\n",
    "\n",
    "http_response_rf_selected = http_response_rf[['test_log_loss', 'test_aupcr', 'test_balanced_accuracy', 'test_f1', 'test_precision', 'test_recall', 'test_mcc']]\n",
    "http_response_rf_selected.columns = ['Log loss', 'AUPRC', 'BACC', 'F1', 'Precision', 'Recall', 'MCC']\n",
    "\n",
    "http_request_rf_selected = http_request_rf[['test_log_loss', 'test_aupcr', 'test_balanced_accuracy', 'test_f1', 'test_precision', 'test_recall', 'test_mcc']]\n",
    "http_request_rf_selected.columns = ['Log loss', 'AUPRC', 'BACC', 'F1', 'Precision', 'Recall', 'MCC']\n",
    "\n",
    "# Combine data into a single dataframe\n",
    "combined_data = {\n",
    "    't.ex-Graph Baseline': baseline_rf_selected.iloc[0].values,\n",
    "    'HTTP Response Headers': http_response_rf_selected.iloc[0].values,\n",
    "    'HTTP Request Headers': http_request_rf_selected.iloc[0].values\n",
    "}\n",
    "combined_df = pd.DataFrame(combined_data, index=['Log loss', 'AUPRC', 'BACC', 'F1', 'Precision', 'Recall', 'MCC'])\n",
    "\n",
    "# Modify the combined dataframe\n",
    "combined_df.loc['Log loss'] = 1 - combined_df.loc['Log loss']\n",
    "combined_df.rename(index={'Log loss': '1 - Log loss', 'F1': 'F1-Score'}, inplace=True)\n",
    "\n",
    "# Plotting the combined data with updated values and labels\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Define the lines and colors for each version of Random Forest\n",
    "colors = ['#1f77b4','#2ca02c','#ff7f0e']\n",
    "labels = ['t.ex-Graph Baseline', 'HTTP Response Headers', 'HTTP Request Headers']\n",
    "\n",
    "for label, color in zip(labels, colors):\n",
    "    ax1.plot(combined_df.index, combined_df[label], marker='o', linestyle='-', label=label, color=color)\n",
    "\n",
    "# Set labels, title, and legend for the primary axis\n",
    "ax1.set_xlabel('Metrics', fontsize=14)\n",
    "ax1.set_ylabel('Score', fontsize=14)\n",
    "ax1.set_title('Performance of RF classifiers', fontsize=16)\n",
    "ax1.set_xticks(np.arange(len(combined_df.index)))\n",
    "ax1.set_xticklabels(combined_df.index, fontsize=12)\n",
    "ax1.legend(fontsize=12, loc='lower right')\n",
    "\n",
    "# Create a secondary y-axis to show the metrics\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_yticks([])  # Hide secondary y-axis ticks\n",
    "\n",
    "# Improve layout\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('request_performance_comparison.pdf', bbox_inches='tight', pad_inches=0.1)\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "id": "d2dd8a21db2b9260",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Predicted Labels Comparison",
   "id": "2b18f65da762317e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T19:24:43.924201Z",
     "start_time": "2024-06-12T19:24:43.920520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_models(\n",
    "    models: List[str], X_test: pd.DataFrame, y_test: pd.Series, http_message: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate the performance of multiple classification models on the test\n",
    "    dataset and return the predictions along with the true labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    models : List[str]\n",
    "        A list of model names to be evaluated. Each model should have a\n",
    "        corresponding saved binary file in the specified directory with the\n",
    "        format \"{model_name}_binary.sav\".\n",
    "\n",
    "    X_test : pd.DataFrame\n",
    "        The test dataset containing the feature values.\n",
    "\n",
    "    y_test : pd.Series\n",
    "        The true labels for the test dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the predictions and true labels for each model.\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "\n",
    "    for model_name in models:\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "\n",
    "        filename = f\"{model_name}_BE.sav.gz\"\n",
    "        gzip_path = f\"../models/chrome/08_12_2022/{http_message}/{filename}\"\n",
    "\n",
    "        with gzip.GzipFile(gzip_path, \"rb\") as f:\n",
    "            best_estimator = pickle.load(f)\n",
    "\n",
    "        y_pred_test = best_estimator.predict(X_test)\n",
    "\n",
    "        all_predictions.append(pd.DataFrame({\n",
    "            'model': model_name,\n",
    "            'httpMessageId': X_test.index,\n",
    "            'true_label': y_test.values,\n",
    "            'predicted_label': y_pred_test\n",
    "        }))\n",
    "\n",
    "    predictions_df = pd.concat(all_predictions, ignore_index=True)\n",
    "\n",
    "    return predictions_df"
   ],
   "id": "d931cbc13c57cd6e",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T19:23:07.029840Z",
     "start_time": "2024-06-12T19:23:07.026548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_models_run(\n",
    "    test_data: pd.DataFrame,\n",
    "    http_message: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load test data, evaluate models, and return the predictions and true labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_data_file_path : str\n",
    "        The file path to the test data (without extension).\n",
    "\n",
    "    http_message : str\n",
    "        The type of HTTP message (e.g., 'response' or 'request').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the predictions and true labels for each model.\n",
    "    \"\"\"\n",
    "    # test_data = pd.read_parquet(f\"{test_data_file_path}.parquet.gzip\")\n",
    "    X_test, y_test = test_data.iloc[:, 2:-2], test_data[\"tracker\"]\n",
    "\n",
    "    models = [\n",
    "        \"Logistic_Regression\",\n",
    "        \"Gaussian_NB\",\n",
    "        \"Decision_Tree\",\n",
    "        \"Random_Forest\",\n",
    "        \"Extra_Trees_Classifier\",\n",
    "        \"Ada_Boost\",\n",
    "        \"Gradient_Boosting\",\n",
    "        \"LightGBM\",\n",
    "        \"Hist_GB\",\n",
    "        \"XGBoost\",\n",
    "    ]\n",
    "\n",
    "    predictions_df = test_models(models, X_test, y_test, http_message)\n",
    "\n",
    "    return predictions_df"
   ],
   "id": "c618763a5b21ffca",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T19:26:30.263868Z",
     "start_time": "2024-06-12T19:26:29.992316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chrome_old = pd.read_parquet(\n",
    "    \"../data/processed/chrome/08_12_2022/test_set_featurized_response_BE.parquet.gzip\"\n",
    ")\n",
    "\n",
    "chrome_new = pd.read_parquet('../data/processed/chrome/03_29_2023/merged_data_featurized_response_BE.parquet.gzip')\n",
    "\n",
    "firefox = pd.read_parquet('../data/processed/firefox/08_12_2022/merged_data_featurized_response_BE.parquet.gzip')\n",
    "\n",
    "brave = pd.read_parquet('../data/processed/brave/08_12_2022/merged_data_featurized_response_BE.parquet.gzip')"
   ],
   "id": "779163843daaac7f",
   "execution_count": 37,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T19:28:23.578453Z",
     "start_time": "2024-06-12T19:28:23.568438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chrome_old_request = pd.read_parquet(\n",
    "    \"../data/processed/chrome/08_12_2022/test_set_featurized_request_BE.parquet.gzip\"\n",
    ")"
   ],
   "id": "1d6e5ae52ea367cb",
   "execution_count": 39,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T19:28:32.310454Z",
     "start_time": "2024-06-12T19:28:26.152245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test response classifier models\n",
    "predictions_response = test_models_run(chrome_old, 'response')\n",
    "# Test request classifier models\n",
    "predictions_request = test_models_run(chrome_old_request, 'request')"
   ],
   "id": "c35373fdb7a6ce0c",
   "execution_count": 40,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T19:28:41.348045Z",
     "start_time": "2024-06-12T19:28:41.342406Z"
    }
   },
   "cell_type": "code",
   "source": "predictions_response",
   "id": "d265a7ca3c255168",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T19:46:22.673915Z",
     "start_time": "2024-06-12T19:46:22.284459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add suffixes to distinguish between the classifier types\n",
    "predictions_response = predictions_response.rename(columns={'predicted_label': 'predicted_label_response', 'true_label': 'true_label_response'})\n",
    "predictions_request = predictions_request.rename(columns={'predicted_label': 'predicted_label_request', 'true_label': 'true_label_request'})\n",
    "\n",
    "# Merge predictions on 'httpMessageId' and 'model'\n",
    "merged_predictions = predictions_response.merge(\n",
    "    predictions_request, on=['httpMessageId', 'model'], suffixes=('_response', '_request'))\n",
    "\n",
    "# Identify cases where the response classifier identified positive classes that the request classifier did not\n",
    "positive_cases = merged_predictions[(merged_predictions['true_label_response'] == 1) &\n",
    "                                    (merged_predictions['predicted_label_response'] == 1) &\n",
    "                                    (merged_predictions['predicted_label_request'] == 0)]\n",
    "\n",
    "# Summary table\n",
    "summary_table = positive_cases[['httpMessageId', 'model', 'true_label_response', 'predicted_label_response', 'predicted_label_request']]\n",
    "\n",
    "# Display the summary table\n",
    "summary_table"
   ],
   "id": "d38c8ec8ff68320",
   "execution_count": 58,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T19:36:20.675597Z",
     "start_time": "2024-06-12T19:36:20.668912Z"
    }
   },
   "cell_type": "code",
   "source": "merged_predictions",
   "id": "2e6173c8edee67fb",
   "execution_count": 51,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T19:44:29.420615Z",
     "start_time": "2024-06-12T19:44:06.222459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add a column to classify the combined predictions based on true labels\n",
    "merged_predictions['result'] = merged_predictions.apply(\n",
    "    lambda row: (\n",
    "        'True for both' if row['predicted_label_response'] == 1 and row['predicted_label_request'] == 1 else\n",
    "        'True for response, False for request' if row['predicted_label_response'] == 1 and row['predicted_label_request'] == 0 else\n",
    "        'False for response, True for request' if row['predicted_label_response'] == 0 and row['predicted_label_request'] == 1 else\n",
    "        'False for both'\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Add a column to indicate if the prediction matches the true label\n",
    "merged_predictions['match_true_label'] = merged_predictions.apply(\n",
    "    lambda row: (\n",
    "        'True Positive' if row['true_label_response'] == 1 and row['predicted_label_response'] == 1 else\n",
    "        'True Negative' if row['true_label_response'] == 0 and row['predicted_label_response'] == 0 else\n",
    "        'False Positive' if row['true_label_response'] == 0 and row['predicted_label_response'] == 1 else\n",
    "        'False Negative'\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Create a contingency table that includes the true label matches\n",
    "contingency_table = pd.crosstab(\n",
    "    index=[merged_predictions['result'], merged_predictions['match_true_label']],\n",
    "    columns='count'\n",
    ").reset_index()\n",
    "\n",
    "# Display the contingency table\n",
    "contingency_table"
   ],
   "id": "2a36c625f1cf49a",
   "execution_count": 55,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T19:44:33.100827Z",
     "start_time": "2024-06-12T19:44:32.853653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot to visualize the counts for each combination of predictions and true label matches\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='result', y='count', hue='match_true_label', data=contingency_table, palette='viridis')\n",
    "plt.title('Counts of Prediction Combinations from Response and Request Classifiers with True Labels')\n",
    "plt.xlabel('Prediction Combination')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='True Label Match')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "2114500b5d065851",
   "execution_count": 56,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T19:49:23.787755Z",
     "start_time": "2024-06-12T19:49:23.781739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_predictions(merged_predictions: pd.DataFrame, selected_classifier: str = None):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the predictions, optionally filtering by a specific classifier,\n",
    "    and plot the results as percentages.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    merged_predictions : pd.DataFrame\n",
    "        The dataframe containing merged predictions from response and request classifiers.\n",
    "    selected_classifier : str, optional\n",
    "        The name of the classifier to filter by. If None, analyze all classifiers.\n",
    "    \"\"\"\n",
    "    # Filter the dataframe based on the selected classifier\n",
    "    if selected_classifier:\n",
    "        merged_predictions = merged_predictions[merged_predictions['model'] == selected_classifier]\n",
    "\n",
    "    # Add a column to classify the combined predictions based on true labels\n",
    "    merged_predictions['result'] = merged_predictions.apply(\n",
    "        lambda row: (\n",
    "            'True for both' if row['predicted_label_response'] == 1 and row['predicted_label_request'] == 1 else\n",
    "            'True for response, False for request' if row['predicted_label_response'] == 1 and row['predicted_label_request'] == 0 else\n",
    "            'False for response, True for request' if row['predicted_label_response'] == 0 and row['predicted_label_request'] == 1 else\n",
    "            'False for both'\n",
    "        ), axis=1\n",
    "    )\n",
    "\n",
    "    # Add a column to indicate if the prediction matches the true label\n",
    "    merged_predictions['match_true_label'] = merged_predictions.apply(\n",
    "        lambda row: (\n",
    "            'True Positive' if row['true_label_response'] == 1 and row['predicted_label_response'] == 1 else\n",
    "            'True Negative' if row['true_label_response'] == 0 and row['predicted_label_response'] == 0 else\n",
    "            'False Positive' if row['true_label_response'] == 0 and row['predicted_label_response'] == 1 else\n",
    "            'False Negative'\n",
    "        ), axis=1\n",
    "    )\n",
    "\n",
    "    # Create a contingency table that includes the true label matches\n",
    "    contingency_table = pd.crosstab(\n",
    "        index=[merged_predictions['result'], merged_predictions['match_true_label']],\n",
    "        columns='count'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate the total count for each result type to compute percentages\n",
    "    total_counts = contingency_table.groupby('result')['count'].sum().reset_index()\n",
    "    total_counts.columns = ['result', 'total']\n",
    "\n",
    "    # Merge the total counts back into the contingency table\n",
    "    contingency_table = contingency_table.merge(total_counts, on='result')\n",
    "\n",
    "    # Calculate the percentage for each combination\n",
    "    contingency_table['percentage'] = (contingency_table['count'] / contingency_table['total']) * 100\n",
    "\n",
    "    # Display the contingency table\n",
    "    print(contingency_table)\n",
    "\n",
    "    # Plot to visualize the percentages for each combination of predictions and true label matches\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(x='result', y='percentage', hue='match_true_label', data=contingency_table, palette='viridis')\n",
    "    plt.title(f'Percentages of Prediction Combinations from Response and Request Classifiers with True Labels\\n(Classifier: {selected_classifier if selected_classifier else \"All\"})')\n",
    "    plt.xlabel('Prediction Combination')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='True Label Match')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "28947ec20fad6056",
   "execution_count": 62,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T19:49:39.961283Z",
     "start_time": "2024-06-12T19:49:37.302041Z"
    }
   },
   "cell_type": "code",
   "source": "analyze_predictions(merged_predictions, selected_classifier='Extra_Trees_Classifier')",
   "id": "4c9b7641aee32fb0",
   "execution_count": 63,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:25:55.704906Z",
     "start_time": "2024-06-12T20:25:55.700505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_additional_metrics(merged_predictions: pd.DataFrame, selected_classifier: str = None):\n",
    "    \"\"\"\n",
    "    Calculate additional metrics for a given classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    merged_predictions : pd.DataFrame\n",
    "        The dataframe containing merged predictions from response and request classifiers.\n",
    "    selected_classifier : str, optional\n",
    "        The name of the classifier to filter by. If None, analyze all classifiers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing the calculated metrics.\n",
    "    \"\"\"\n",
    "    # Filter the dataframe based on the selected classifier\n",
    "    if selected_classifier:\n",
    "        merged_predictions = merged_predictions[merged_predictions['model'] == selected_classifier]\n",
    "\n",
    "    # Calculate how often the response classifier correctly predicted the label while the request classifier did not\n",
    "    correct_response_incorrect_request = merged_predictions[\n",
    "        (merged_predictions['true_label_response'] == merged_predictions['predicted_label_response']) & \n",
    "        (merged_predictions['true_label_response'] != merged_predictions['predicted_label_request'])\n",
    "    ].shape[0]\n",
    "\n",
    "    # Calculate how often the response classifier predicted \"yes\" (1) when the true label was 0\n",
    "    false_positive_response = merged_predictions[\n",
    "        (merged_predictions['true_label_response'] == 0) & \n",
    "        (merged_predictions['predicted_label_response'] == 1)\n",
    "    ].shape[0]\n",
    "\n",
    "    total_predictions = merged_predictions.shape[0]\n",
    "\n",
    "    metrics = {\n",
    "        'correct_response_incorrect_request': correct_response_incorrect_request,\n",
    "        'false_positive_response': false_positive_response,\n",
    "        'total_predictions': total_predictions,\n",
    "        'correct_response_incorrect_request_percentage': (correct_response_incorrect_request / total_predictions) * 100,\n",
    "        'false_positive_response_percentage': (false_positive_response / total_predictions) * 100\n",
    "    }\n",
    "\n",
    "    return metrics"
   ],
   "id": "b69d2897f3b98f65",
   "execution_count": 64,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:26:08.641258Z",
     "start_time": "2024-06-12T20:26:08.564816Z"
    }
   },
   "cell_type": "code",
   "source": "calculate_additional_metrics(merged_predictions, selected_classifier='Logistic_Regression')",
   "id": "abaae46316ae67f7",
   "execution_count": 65,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Radar Plot For Cross-Browser Comparison",
   "id": "f2552e6cc70932b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T21:14:32.035158Z",
     "start_time": "2024-06-12T21:14:32.027664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def radar_factory(num_vars, frame='circle'):\n",
    "    \"\"\"\n",
    "    Create a radar chart with `num_vars` axes.\n",
    "\n",
    "    This function creates a RadarAxes projection and registers it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_vars : int\n",
    "        Number of variables for radar chart.\n",
    "    frame : {'circle', 'polygon'}\n",
    "        Shape of frame surrounding axes.\n",
    "\n",
    "    \"\"\"\n",
    "    # calculate evenly-spaced axis angles\n",
    "    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n",
    "\n",
    "    class RadarTransform(PolarAxes.PolarTransform):\n",
    "\n",
    "        def transform_path_non_affine(self, path):\n",
    "            # Paths with non-unit interpolation steps correspond to gridlines,\n",
    "            # in which case we force interpolation (to defeat PolarTransform's\n",
    "            # autoconversion to circular arcs).\n",
    "            if path._interpolation_steps > 1:\n",
    "                path = path.interpolated(num_vars)\n",
    "            return Path(self.transform(path.vertices), path.codes)\n",
    "\n",
    "    class RadarAxes(PolarAxes):\n",
    "\n",
    "        name = 'radar'\n",
    "        PolarTransform = RadarTransform\n",
    "\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            # rotate plot such that the first axis is at the top\n",
    "            self.set_theta_zero_location('N')\n",
    "\n",
    "        def fill(self, *args, closed=True, **kwargs):\n",
    "            \"\"\"Override fill so that line is closed by default\"\"\"\n",
    "            return super().fill(closed=closed, *args, **kwargs)\n",
    "\n",
    "        def plot(self, *args, **kwargs):\n",
    "            \"\"\"Override plot so that line is closed by default\"\"\"\n",
    "            lines = super().plot(*args, **kwargs)\n",
    "            for line in lines:\n",
    "                self._close_line(line)\n",
    "\n",
    "        def _close_line(self, line):\n",
    "            x, y = line.get_data()\n",
    "            # FIXME: markers at x[0], y[0] get doubled-up\n",
    "            if x[0] != x[-1]:\n",
    "                x = np.append(x, x[0])\n",
    "                y = np.append(y, y[0])\n",
    "                line.set_data(x, y)\n",
    "\n",
    "        def set_varlabels(self, labels):\n",
    "            lines, texts = self.set_thetagrids(np.degrees(theta), labels)\n",
    "            for text in texts:\n",
    "                text.set_zorder(100)\n",
    "\n",
    "        def _gen_axes_patch(self):\n",
    "            # The Axes patch must be centered at (0.5, 0.5) and of radius 0.5\n",
    "            # in axes coordinates.\n",
    "            if frame == 'circle':\n",
    "                return Circle((0.5, 0.5), 0.5)\n",
    "            elif frame == 'polygon':\n",
    "                return RegularPolygon((0.5, 0.5), num_vars,\n",
    "                                      radius=.5, edgecolor=\"k\")\n",
    "            else:\n",
    "                raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
    "\n",
    "        def _gen_axes_spines(self):\n",
    "            if frame == 'circle':\n",
    "                return super()._gen_axes_spines()\n",
    "            elif frame == 'polygon':\n",
    "                # spine_type must be 'left'/'right'/'top'/'bottom'/'circle'.\n",
    "                spine = Spine(axes=self,\n",
    "                              spine_type='circle',\n",
    "                              path=Path.unit_regular_polygon(num_vars))\n",
    "                # unit_regular_polygon gives a polygon of radius 1 centered at\n",
    "                # (0, 0) but we want a polygon of radius 0.5 centered at (0.5,\n",
    "                # 0.5) in axes coordinates.\n",
    "                spine.set_transform(Affine2D().scale(.5).translate(.5, .5)\n",
    "                                    + self.transAxes)\n",
    "                return {'polar': spine}\n",
    "            else:\n",
    "                raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
    "\n",
    "    register_projection(RadarAxes)\n",
    "    return theta"
   ],
   "id": "585321fd6a9d21e1",
   "execution_count": 67,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:55:53.295526Z",
     "start_time": "2024-06-13T07:55:53.289698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_csv(file_path, headers):\n",
    "    \"\"\"Load CSV data and return a dictionary of model:metrics.\"\"\"\n",
    "    data = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            model_name = row['']\n",
    "            metrics = [float(row[col]) for col in headers]\n",
    "            data[model_name] = metrics\n",
    "    return data\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    path = '../models/result_metrics/'\n",
    "\n",
    "    # Assuming all CSV files are in the same directory called 'csv_directory'\n",
    "    csv_files = [f'{path}chrome_08_12_2022_test_set_response.csv', f'{path}chrome_03_29_2023_merged_data_response.csv', f'{path}firefox_08_12_2022_merged_data_response.csv', f'{path}brave_08_12_2022_merged_data_response.csv']\n",
    "\n",
    "    # The headers for metrics we're interested in.\n",
    "    headers = ['test_accuracy','test_log_loss','test_auc','test_aupcr','test_balanced_accuracy','test_f1','test_precision','test_recall','test_mcc']\n",
    "\n",
    "    aggregated_data = {}\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        file_data = load_csv(csv_file, headers)\n",
    "        for model, metrics in file_data.items():\n",
    "            if model not in aggregated_data:\n",
    "                aggregated_data[model] = []\n",
    "            aggregated_data[model].append(metrics)\n",
    "\n",
    "    # Normalize test_log_loss values\n",
    "    # Get the global min and max test_log_loss values\n",
    "    all_log_loss_values = [metrics[headers.index('test_log_loss')] for all_metrics in aggregated_data.values() for metrics in all_metrics]\n",
    "    min_val = min(all_log_loss_values)\n",
    "    max_val = max(all_log_loss_values)\n",
    "\n",
    "    # Normalize test_log_loss values using the global min and max\n",
    "    for model, all_metrics in aggregated_data.items():\n",
    "        for metrics in all_metrics:\n",
    "            index = headers.index('test_log_loss')\n",
    "            metrics[index] = (metrics[index] - min_val) / (max_val - min_val) if max_val != min_val else 0.5\n",
    "\n",
    "    # Calculate 1 - log-loss score\n",
    "    # one_minus_log_loss_scores = {}\n",
    "    # for model, all_metrics in aggregated_data.items():\n",
    "    #     log_losses = [metrics[headers.index('test_log_loss')] for metrics in all_metrics]\n",
    "    #     avg_log_loss = sum(log_losses) / len(log_losses)  # Average log-loss for the model\n",
    "    #     one_minus_log_loss_scores[model] = 1 - avg_log_loss\n",
    "\n",
    "    # Convert dictionary to the desired format\n",
    "    # final_data = [headers + ['1_minus_log_loss']]\n",
    "    # for model, all_metrics in aggregated_data.items():\n",
    "    #     avg_metrics = [sum(metric)/len(metric) for metric in zip(*all_metrics)]\n",
    "    #     final_data.append([model] + avg_metrics + [one_minus_log_loss_scores[model]])\n",
    "        \n",
    "    # Convert dictionary to the desired format\n",
    "    final_data = [headers]\n",
    "    for model, all_metrics in aggregated_data.items():\n",
    "        final_data.append((model, all_metrics))\n",
    "\n",
    "    return final_data"
   ],
   "id": "85ee7a7a7a94a140",
   "execution_count": 124,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T21:16:07.920127Z",
     "start_time": "2024-06-12T21:16:07.917408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_models(model_list, data):\n",
    "    \"\"\"\n",
    "    Filter the data for the given list of model names.\n",
    "\n",
    "    Parameters:\n",
    "    - model_list: List of model names to filter.\n",
    "    - data: The data structure to filter.\n",
    "\n",
    "    Returns:\n",
    "    - A subset of the data containing only the given models.\n",
    "    \"\"\"\n",
    "    filtered_data = [data[0]]  # add the headers\n",
    "    for model_data in data[1:]:\n",
    "        if model_data[0] in model_list:\n",
    "            filtered_data.append(model_data)\n",
    "    return filtered_data"
   ],
   "id": "e0938bb88e8db2e5",
   "execution_count": 69,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T21:16:12.694460Z",
     "start_time": "2024-06-12T21:16:12.691329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def adjust_label_alignment(ax):\n",
    "    \"\"\"Adjust the alignment of radar spine labels.\"\"\"\n",
    "    # Get the labels\n",
    "    labels = ax.get_xticklabels()\n",
    "    ticks = ax.get_xticks()\n",
    "\n",
    "    for label, tick in zip(labels, ax.xaxis.get_major_ticks()):\n",
    "        # Get the label's position\n",
    "        x, y = label.get_position()\n",
    "\n",
    "        # Based on its position, set the alignment\n",
    "        if x == 0:\n",
    "            label.set_horizontalalignment('center')\n",
    "        elif x < 3:\n",
    "            label.set_horizontalalignment('right')\n",
    "        else:\n",
    "            label.set_horizontalalignment('left')"
   ],
   "id": "9b6c73ed94d31120",
   "execution_count": 70,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T08:00:34.352151Z",
     "start_time": "2024-06-13T08:00:33.469042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filtered_models = ['Logistic_Regression', 'Extra_Trees_Classifier', 'XGBoost']\n",
    "data = filter_models(filtered_models, load_data())\n",
    "#spoke_labels = data.pop(0)\n",
    "data.pop(0)\n",
    "spoke_labels = ['Accuracy', '1 - Log-loss', 'ROC-AUC', 'AUPRC', 'BACC', 'F1-Score', 'Precision', 'Recall', 'MCC']\n",
    "# selected_indices = [0, 1, 2, 4, 5, 6]  # Indices of the selected metrics\n",
    "# spoke_labels = ['1 - Log-loss', 'ROC-AUC', 'AUPRC', 'BACC', 'F1-Score', 'MCC']\n",
    "\n",
    "N = len(spoke_labels)\n",
    "\n",
    "theta = radar_factory(N, frame='polygon')\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(17, 4), nrows=1, ncols=3,\n",
    "                        subplot_kw=dict(projection='radar'))\n",
    "fig.subplots_adjust(wspace=0.15, hspace=0.20, top=0.80, bottom=0.05)\n",
    "plt.rcParams.update({'mathtext.default': 'regular'})\n",
    "\n",
    "colors = [\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#F0E442\"]\n",
    "linestyles = [\"-\", \"--\", \"-.\", \":\", \"-\", \"--\", \"-.\"]\n",
    "\n",
    "# Plot the four cases from the example data on separate axes\n",
    "for ax, (title, case_data) in zip(axs.flat, data):\n",
    "    # modify grid and background\n",
    "    ax.set_rgrids([0.2, 0.4, 0.6, 0.8])\n",
    "    ax.grid(color='lightgrey')  # Set the gridline color to light grey\n",
    "    ax.set_facecolor('none')\n",
    "    ax.spines['polar'].set_edgecolor('lightgrey')\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    # set title\n",
    "    ax.set_title(title.replace('_', ' '), weight='bold', position=(0.5, 1),\n",
    "                 horizontalalignment='center')\n",
    "\n",
    "    # set labels and label position\n",
    "    ax.set_varlabels(spoke_labels)\n",
    "    adjust_label_alignment(ax)\n",
    "\n",
    "    ticks = ax.xaxis.get_major_ticks()\n",
    "    for tick in ticks:\n",
    "        tick.set_pad(-2)\n",
    "\n",
    "    # plot data\n",
    "    for d, color, ls in zip(case_data, colors, linestyles):\n",
    "        d = d[:1] + [1 - d[1]] + d[2:]  # Convert log-loss to 1 - log-loss on the fly\n",
    "        ax.plot(theta, d, color=color, linestyle=ls)\n",
    "        # ax.fill(theta, d, facecolor=color, alpha=0.25, label='_nolegend_')\n",
    "\n",
    "\n",
    "# add legend relative to top-left plot\n",
    "labels = ('$Chrome_{22}$', '$Chrome_{23}$', '$Firefox_{22}$', '$Brave_{22}$')\n",
    "legend = axs[2].legend(labels, loc=(1.05, 0.5), bbox_to_anchor=(1.05, 0.7))\n",
    "legend.set_title('Browser')\n",
    "# Set the legend properties for transparent background and light grey border\n",
    "legend.get_frame().set_facecolor('none')  # Transparent background\n",
    "legend.get_frame().set_edgecolor('lightgrey')  # Light grey border\n",
    "legend.get_frame().set_linewidth(1.0)  # Set the linewidth to 1.0\n",
    "\n",
    "fig.text(0.5, 0.94, 'Cross-browser and longitudinal performance of selected classifiers',\n",
    "         verticalalignment='bottom',\n",
    "         horizontalalignment='center', color='black',\n",
    "         size=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('spider.pdf', format='pdf', bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "plt.show()"
   ],
   "id": "8949c1298cfd849e",
   "execution_count": 128,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ECDF and Feature Similarity",
   "id": "146f68eedac66d85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T12:18:59.039684Z",
     "start_time": "2024-06-13T12:18:57.780072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chrome_old = pd.read_parquet(\n",
    "    \"../data/processed/chrome/08_12_2022/train_set_processed_response.parquet.gzip\", engine='pyarrow', dtype_backend='pyarrow'\n",
    ")\n",
    "\n",
    "chrome_new = pd.read_parquet('../data/processed/chrome/03_29_2023/merged_data_processed_response.parquet.gzip', engine='pyarrow', dtype_backend='pyarrow')\n",
    "\n",
    "firefox = pd.read_parquet('../data/processed/firefox/08_12_2022/merged_data_processed_response.parquet.gzip', engine='pyarrow', dtype_backend='pyarrow')\n",
    "\n",
    "brave = pd.read_parquet('../data/processed/brave/08_12_2022/merged_data_processed_response.parquet.gzip', engine='pyarrow', dtype_backend='pyarrow')"
   ],
   "id": "bcedcfa680c83578",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T13:27:28.922039Z",
     "start_time": "2024-06-13T13:27:23.786607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Plotting\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "cl_values = chrome_old[[\"content-length\", \"tracker\"]].dropna().astype('Int32')\n",
    "cl_values_chrome = chrome_new[[\"content-length\", \"tracker\"]].dropna().astype('Int32')\n",
    "\n",
    "# Add row identifiers\n",
    "fig.text(-0.025, 0.75, '(A)', ha='center', va='center', fontsize=20)\n",
    "fig.text(-0.025, 0.25, '(B)', ha='center', va='center', fontsize=20)\n",
    "\n",
    "# ECDF plot for Chrome (example data)\n",
    "sns.ecdfplot(data=cl_values[cl_values['content-length'] < 10000], x=\"content-length\", hue='tracker', ax=axes[0, 0], color='blue')\n",
    "axes[0, 0].set_title('$Chrome_{22}$: ECDF of Content-Length Header', fontsize=14)\n",
    "axes[0, 0].set_xlabel('Content-Length Value', fontsize=14)\n",
    "axes[0, 0].set_ylabel('Cumulative Probability', fontsize=14)\n",
    "labels = ('Tracker', 'Non-Tracker')\n",
    "# legend = axs[2].legend(labels, loc=(1.05, 0.5), bbox_to_anchor=(1.05, 0.7))\n",
    "# handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "axes[0, 0].legend(labels=labels, title='Classification')\n",
    "axes[0, 0].tick_params(axis='y', labelsize=12)\n",
    "axes[0, 0].tick_params(axis='x', labelsize=12)\n",
    "\n",
    "# axes[0, 0].legend(title='Classification')\n",
    "\n",
    "# ECDF plot for Brave (example data)\n",
    "sns.ecdfplot(data=cl_values_chrome[cl_values_chrome['content-length'] < 10000], x=\"content-length\", hue='tracker', ax=axes[0, 1], color='orange')\n",
    "axes[0, 1].set_title('$Chrome_{23}$: ECDF of Content-Length Header', fontsize=14)\n",
    "axes[0, 1].set_xlabel('Content-Length Value', fontsize=14)\n",
    "axes[0, 1].set_ylabel('Cumulative Probability', fontsize=14)\n",
    "axes[0, 1].legend(labels=labels, title='Classification')\n",
    "axes[0, 1].tick_params(axis='y', labelsize=12)\n",
    "axes[0, 1].tick_params(axis='x', labelsize=12)\n",
    "\n",
    "# Relative Frequency plot for Chrome vs Firefox\n",
    "feature = 'x-xss-protection'\n",
    "# feature = 'cross-origin-resource-policy'\n",
    "\n",
    "df_chrome_firefox = pd.DataFrame({\n",
    "    feature: np.concatenate(\n",
    "        (chrome_old.loc[:, feature], firefox.loc[:, feature])),\n",
    "    'set': ['Chrome'] * chrome_old.shape[0] + ['Firefox'] * firefox.shape[0]\n",
    "})\n",
    "\n",
    "# Drop rows with NA values\n",
    "df_chrome_firefox = df_chrome_firefox.dropna(subset=[feature])\n",
    "\n",
    "top_n = 5\n",
    "# Calculate frequencies\n",
    "frequency = df_chrome_firefox.groupby(['set', feature]).size().reset_index(name='count')\n",
    "\n",
    "# Identify top N most frequent values across both sets\n",
    "top_values = frequency.groupby(feature)['count'].sum().nlargest(top_n).index\n",
    "\n",
    "# Filter data to include only top N values\n",
    "df_chrome_firefox = df_chrome_firefox[df_chrome_firefox[feature].isin(top_values)]\n",
    "\n",
    "# Recalculate relative frequencies for filtered data\n",
    "relative_freq = df_chrome_firefox.groupby(['set', feature]).size().reset_index(name='count')\n",
    "total_counts = relative_freq.groupby('set')['count'].transform('sum')\n",
    "relative_freq['relative_frequency'] = relative_freq['count'] / total_counts\n",
    "relative_freq.sort_values(by='relative_frequency', ascending=False, inplace=True)\n",
    "sns.barplot(data=relative_freq, x=feature, y='relative_frequency', hue='set', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Feature Similarity between Chrome and Firefox', fontsize=14)\n",
    "axes[1, 0].set_xlabel('Feature Values', fontsize=14)\n",
    "axes[1, 0].set_ylabel('Relative Frequency', fontsize=14)\n",
    "labels = ('$Chrome_{22}$', '$Firefox_{23}$')\n",
    "axes[1, 0].legend(labels=labels, title='Browser')\n",
    "axes[1, 0].tick_params(axis='y', labelsize=12)\n",
    "axes[1, 0].tick_params(axis='x', labelsize=12)\n",
    "\n",
    "# Relative Frequency plot for Chrome vs Brave\n",
    "df_chrome_brave = pd.DataFrame({\n",
    "    feature: np.concatenate(\n",
    "        (chrome_old.loc[:, feature], brave.loc[:, feature])),\n",
    "    'set': ['Chrome'] * chrome_old.shape[0] + ['Brave'] * brave.shape[0]\n",
    "})\n",
    "\n",
    "# Drop rows with NA values\n",
    "df_chrome_brave = df_chrome_brave.dropna(subset=[feature])\n",
    "\n",
    "frequency_brave = df_chrome_brave.groupby(['set', feature]).size().reset_index(name='count')\n",
    "\n",
    "# Identify top N most frequent values across both sets\n",
    "top_values_brave = frequency_brave.groupby(feature)['count'].sum().nlargest(top_n).index\n",
    "\n",
    "# Filter data to include only top N values\n",
    "df_chrome_brave = df_chrome_brave[df_chrome_brave[feature].isin(top_values_brave)]\n",
    "\n",
    "relative_freq_brave = df_chrome_brave.groupby(['set', feature]).size().reset_index(name='count')\n",
    "total_counts_brave = relative_freq_brave.groupby('set')['count'].transform('sum')\n",
    "relative_freq_brave['relative_frequency'] = relative_freq_brave['count'] / total_counts_brave\n",
    "relative_freq_brave.sort_values(by='relative_frequency', ascending=False, inplace=True)\n",
    "sns.barplot(data=relative_freq_brave, x=feature, y='relative_frequency', hue='set', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Feature Similarity between Chrome and Brave', fontsize=14)\n",
    "axes[1, 1].set_xlabel('Feature Values', fontsize=14)\n",
    "axes[1, 1].set_ylabel('Relative Frequency', fontsize=14)\n",
    "labels = ('$Chrome_{22}$', '$Brave_{23}$')\n",
    "axes[1, 1].legend(labels=labels, title='Browser')\n",
    "axes[1, 1].tick_params(axis='y', labelsize=12)\n",
    "axes[1, 1].tick_params(axis='x', labelsize=12)\n",
    "\n",
    "# legend.get_frame().set_facecolor('none')  # Transparent background\n",
    "# legend.get_frame().set_edgecolor('lightgrey')  # Light grey border\n",
    "# legend.get_frame().set_linewidth(1.0)  # Set the linewidth to 1.0\n",
    "plt.tight_layout()\n",
    "# legend color had to be changed manually in a third-party program as we coudnt figure out how\n",
    "plt.savefig('content_length_comparison.svg', format='svg', bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()"
   ],
   "id": "92ca0be9481fd517",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We had to change the colors of the lines for the upper two plots manually (tracker and non-tracker) because multiple (should be working) attempts did not work. The same applies for the browser legend in the lower two plots where the color was not accurately shown.   ",
   "id": "6fed64340cba8a3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Feature Vector Similarity",
   "id": "c3d145259bb7ced0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:28:07.157219Z",
     "start_time": "2024-06-13T19:28:07.070077Z"
    }
   },
   "cell_type": "code",
   "source": "chrome_data = pd.read_parquet('../data/processed/chrome/08_12_2022/train_set_featurized_response_BE.parquet.gzip').iloc[:, 2:]",
   "id": "fe8c4bafebcdde29",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:28:08.106174Z",
     "start_time": "2024-06-13T19:28:07.931346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chrome_new = pd.read_parquet('../data/processed/chrome/03_29_2023/merged_data_featurized_response_BE.parquet.gzip', engine='pyarrow', dtype_backend='pyarrow').iloc[:, 2:]\n",
    "firefox = pd.read_parquet('../data/processed/firefox/08_12_2022/merged_data_featurized_response_BE.parquet.gzip', engine='pyarrow', dtype_backend='pyarrow').iloc[:, 2:]\n",
    "brave = pd.read_parquet('../data/processed/brave/08_12_2022/merged_data_featurized_response_BE.parquet.gzip', engine='pyarrow', dtype_backend='pyarrow').iloc[:, 2:]"
   ],
   "id": "211e89fa1ae461b3",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:28:09.578930Z",
     "start_time": "2024-06-13T19:28:09.514721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chrome_data = chrome_data.drop(columns=['httpMessageId'])\n",
    "chrome_new = chrome_new.drop(columns=['httpMessageId'])\n",
    "firefox_data = firefox.drop(columns=['httpMessageId'])\n",
    "brave_data = brave.drop(columns=['httpMessageId'])"
   ],
   "id": "e067e42b91aca004",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:28:15.726531Z",
     "start_time": "2024-06-13T19:28:11.994658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chrome_old_group = chrome_data.iloc[:, :-1].drop_duplicates()\n",
    "chrome_new_group = chrome_new.iloc[:, :-1].drop_duplicates()\n",
    "firefox_group = firefox_data.iloc[:, :-1].drop_duplicates()\n",
    "brave_group = brave_data.iloc[:, :-1].drop_duplicates()"
   ],
   "id": "6c78dc9d416857a1",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:28:18.505824Z",
     "start_time": "2024-06-13T19:28:18.502802Z"
    }
   },
   "cell_type": "code",
   "source": "usage_counts = chrome_old_group.copy()",
   "id": "bd6a9607836aba13",
   "execution_count": 33,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:28:19.348928Z",
     "start_time": "2024-06-13T19:28:19.305731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature_vectors = chrome_data.drop(columns=['tracker'])\n",
    "tracker_labels = chrome_data['tracker']"
   ],
   "id": "92ab75421ce502d7",
   "execution_count": 34,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:31:25.546772Z",
     "start_time": "2024-06-13T19:31:25.324822Z"
    }
   },
   "cell_type": "code",
   "source": "feature_vectors_with_labels = pd.concat([tracker_labels, chrome_old_group], axis=1)",
   "id": "89e1b7f7e711e02d",
   "execution_count": 37,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:31:39.343282Z",
     "start_time": "2024-06-13T19:31:38.326834Z"
    }
   },
   "cell_type": "code",
   "source": "unique_counts = feature_vectors_with_labels.groupby(list(feature_vectors.columns) + ['tracker']).size().reset_index(name='count')",
   "id": "c5acf22b620585c4",
   "execution_count": 39,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:35:18.967962Z",
     "start_time": "2024-06-13T19:35:17.978539Z"
    }
   },
   "cell_type": "code",
   "source": "feature_vectors_with_labels.value_counts()",
   "id": "a6c18e4f95a91c0f",
   "execution_count": 53,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
